{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,Subset, DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "from torchvision import transforms, datasets\n",
    "from pathlib import Path\n",
    "# split validation set into new train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "#plot examples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import copy\n",
    "\n",
    "from baselineCNN import *\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('./../wildfire-prediction-dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = dataset_path / 'train'\n",
    "val_path = dataset_path / 'valid'\n",
    "test_path = dataset_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(test_path, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(pretrain_path, val_path, test_path, transforms):\n",
    "    \n",
    "    pretrain_dataset = datasets.ImageFolder(pretrain_path, transform=transforms['pretrain'])\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=transforms['valid'])\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=transforms['test'])\n",
    "    train_idx, validation_idx = train_test_split(np.arange(len(val_dataset)),\n",
    "                                             test_size=0.2,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=val_dataset.targets)\n",
    "    train_dataset = Subset(val_dataset, train_idx)\n",
    "    val_dataset = Subset(val_dataset, validation_idx)\n",
    "    \n",
    "    return pretrain_dataset, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32  \n",
    "\n",
    "\n",
    "# Data transformations\n",
    "data_transforms = {\n",
    "    'pretrain': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "unlabeled, train_dataset, val_dataset, test_dataset = get_all_datasets(pretrain_path, val_path, test_path, data_transforms)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "unlabeled_loader = DataLoader(unlabeled, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(data_loader):\n",
    "            x = x.to(device).half()  # Convert to float16\n",
    "            y = y.to(device)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                y_hat = model(x)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += (y == y_hat.argmax(1)).sum().item()\n",
    "    return losses, correct_predictions\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, loss_fn, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x, y in tqdm(data_loader):\n",
    "        x = x.float().to(device).half()  # Convert to float16\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):  # Use automatic mixed precision\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "def train_model(model, num_epochs, optimizer, train_loader, val_loader, criterion, device, best_model_path):\n",
    "    model.train()\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, criterion, device)\n",
    "        val_loss, correct_predictions = validate(model, val_loader, criterion, device)\n",
    "        val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "            }, best_model_path)\n",
    "        \n",
    "        print(f'Train Loss: {np.mean(train_loss):.4f} Validation Loss: {np.mean(val_loss):.4f} Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def pseudo_label_dataset(model, unlabeled_loader, device, confidence_threshold=0.95):\n",
    "    model.eval()\n",
    "    pseudo_inputs = []\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(unlabeled_loader, desc=\"Generating pseudo-labels\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_probs, preds = torch.max(probabilities, 1)\n",
    "            \n",
    "            # Select samples with high confidence\n",
    "            confident_mask = max_probs >= confidence_threshold\n",
    "            if confident_mask.sum() > 0:\n",
    "                confident_inputs = inputs[confident_mask].cpu()\n",
    "                confident_labels = preds[confident_mask].cpu()\n",
    "                \n",
    "                pseudo_inputs.append(confident_inputs)\n",
    "                pseudo_labels.append(confident_labels)\n",
    "    \n",
    "    if not pseudo_inputs:\n",
    "        return None\n",
    "    \n",
    "    # Combine all selected samples\n",
    "    pseudo_inputs = torch.cat(pseudo_inputs, 0)\n",
    "    pseudo_labels = torch.cat(pseudo_labels, 0)\n",
    "    \n",
    "    pseudo_dataset = TensorDataset(pseudo_inputs, pseudo_labels.long())\n",
    "    print(f\"Generated {len(pseudo_dataset)} pseudo-labeled samples from {len(unlabeled_loader.dataset)} with a confidence threshold = {confidence_threshold}\")\n",
    "    return pseudo_dataset\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    # Convert all labels to a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3832431/1246028678.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"baseline.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline = baseline.to(device)\n",
    "checkpoint = torch.load(\"baseline.pth\")\n",
    "baseline.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:15<00:00, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1906 Test Accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "baseline.eval()\n",
    "test_loss, correct_predictions = validate(baseline, test_data_loader, criterion, device)\n",
    "base_accuracy = correct_predictions / len(test_dataset)\n",
    "print(f'Test Loss: {np.mean(test_loss):.4f} Test Accuracy: {base_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.95  # Initial confidence threshold\n",
    "initial_acc = base_accuracy\n",
    "num_iterations = 3\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabeled = Subset(unlabeled, list(range(10)))\n",
    "# train_dataset = Subset(train_dataset, list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pseudo-labeling iteration :  1/3 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels:   0%|          | 0/946 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 946/946 [04:16<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16381 pseudo-labeled samples from 30250 with a confidence threshold = 0.95\n",
      " actual dataset number of samples 21421\n",
      "Training with 21421 samples (5040 original labeled + 16381 pseudo-labeled)\n",
      "Remaining unlabeled samples: 13869\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [02:09<00:00,  5.18it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1742 Validation Loss: 0.3127 Validation Accuracy: 0.8659\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:58<00:00,  5.64it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0553 Validation Loss: 0.2709 Validation Accuracy: 0.9111\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:57<00:00,  5.72it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0527 Validation Loss: 0.3314 Validation Accuracy: 0.8714\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [02:13<00:00,  5.01it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0337 Validation Loss: 0.2435 Validation Accuracy: 0.9341\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:54<00:00,  5.87it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0328 Validation Loss: 0.5845 Validation Accuracy: 0.7333\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:53<00:00,  5.92it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0289 Validation Loss: 0.6171 Validation Accuracy: 0.7056\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:52<00:00,  5.96it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0233 Validation Loss: 0.3103 Validation Accuracy: 0.8810\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:53<00:00,  5.88it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0276 Validation Loss: 0.4465 Validation Accuracy: 0.7817\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:52<00:00,  5.95it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0167 Validation Loss: 0.4198 Validation Accuracy: 0.8000\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:54<00:00,  5.87it/s]\n",
      "100%|██████████| 40/40 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0167 Validation Loss: 0.2344 Validation Accuracy: 0.8881\n",
      "start evaluation : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:20<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model performance at iteratio 0 is : 0.8971428571428571 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pseudo-labeling iteration :  2/3 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 434/434 [01:34<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5426 pseudo-labeled samples from 13869 with a confidence threshold = 0.95\n",
      " actual dataset number of samples 26847\n",
      "Training with 26847 samples (5040 original labeled + 21807 pseudo-labeled)\n",
      "Remaining unlabeled samples: 8443\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:15<00:00,  6.18it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1416 Validation Loss: 0.2978 Validation Accuracy: 0.8817\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:36<00:00,  5.35it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0552 Validation Loss: 0.2756 Validation Accuracy: 0.8889\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:31<00:00,  5.54it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0460 Validation Loss: 0.2824 Validation Accuracy: 0.8571\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:49<00:00,  4.95it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0392 Validation Loss: 0.3678 Validation Accuracy: 0.8095\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:18<00:00,  6.06it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0352 Validation Loss: 0.3198 Validation Accuracy: 0.8437\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:13<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0335 Validation Loss: 0.4145 Validation Accuracy: 0.8413\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:11<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0294 Validation Loss: 0.4507 Validation Accuracy: 0.8230\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:13<00:00,  6.28it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0265 Validation Loss: 0.5410 Validation Accuracy: 0.7603\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:12<00:00,  6.31it/s]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0226 Validation Loss: 0.9040 Validation Accuracy: 0.6516\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839/839 [02:15<00:00,  6.21it/s]\n",
      "100%|██████████| 40/40 [00:11<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0217 Validation Loss: 0.6391 Validation Accuracy: 0.7603\n",
      "start evaluation : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:22<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model performance at iteratio 1 is : 0.775079365079365 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pseudo-labeling iteration :  3/3 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 264/264 [01:02<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5771 pseudo-labeled samples from 8443 with a confidence threshold = 0.95\n",
      " actual dataset number of samples 32618\n",
      "Training with 32618 samples (5040 original labeled + 27578 pseudo-labeled)\n",
      "Remaining unlabeled samples: 2672\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:50<00:00,  5.97it/s]\n",
      "100%|██████████| 40/40 [00:12<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3579 Validation Loss: 0.5670 Validation Accuracy: 0.7310\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:44<00:00,  6.20it/s]\n",
      "100%|██████████| 40/40 [00:12<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1127 Validation Loss: 0.6127 Validation Accuracy: 0.6317\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:40<00:00,  6.35it/s]\n",
      "100%|██████████| 40/40 [00:13<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0891 Validation Loss: 0.5822 Validation Accuracy: 0.7413\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:42<00:00,  6.26it/s]\n",
      "100%|██████████| 40/40 [00:14<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0847 Validation Loss: 0.5028 Validation Accuracy: 0.7770\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:38<00:00,  6.42it/s]\n",
      "100%|██████████| 40/40 [00:12<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0776 Validation Loss: 0.6765 Validation Accuracy: 0.6730\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:44<00:00,  6.19it/s]\n",
      "100%|██████████| 40/40 [00:13<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0757 Validation Loss: 0.7984 Validation Accuracy: 0.6778\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:40<00:00,  6.37it/s]\n",
      "100%|██████████| 40/40 [00:13<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0705 Validation Loss: 1.1719 Validation Accuracy: 0.6087\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:40<00:00,  6.36it/s]\n",
      "100%|██████████| 40/40 [00:13<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0656 Validation Loss: 0.8434 Validation Accuracy: 0.6833\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [02:41<00:00,  6.33it/s]\n",
      "100%|██████████| 40/40 [00:13<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0630 Validation Loss: 0.9826 Validation Accuracy: 0.6389\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1020 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "model = baseline\n",
    "\n",
    "# Keep track of original labeled dataset\n",
    "original_train_dataset = train_dataset\n",
    "\n",
    "# Keep track of unlabeled samples and which ones have been pseudo-labeled\n",
    "remaining_unlabeled = list(range(len(unlabeled)))\n",
    "all_pseudo_labeled_indices = set()\n",
    "all_pseudo_labeled_datasets = []\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"\\nPseudo-labeling iteration :  {iteration+1}/{num_iterations} \\n \")\n",
    "    \n",
    "    # Create a loader only for remaining unlabeled data\n",
    "    remaining_unlabeled_dataset = torch.utils.data.Subset(unlabeled, remaining_unlabeled)\n",
    "    unlabeled_loader = DataLoader(remaining_unlabeled_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Generate pseudo-labels for the remaining unlabeled data\n",
    "    pseudo_dataset = pseudo_label_dataset(model, unlabeled_loader, device, confidence_threshold)\n",
    "    \n",
    "\n",
    "    if pseudo_dataset is None or len(pseudo_dataset) == 0:\n",
    "        print(f\"No confident samples found at threshold {confidence_threshold}. Lowering threshold.\")\n",
    "        confidence_threshold *= 0.9  # Reduce threshold\n",
    "\n",
    "    else : \n",
    "        # Store this iteration's pseudo-labeled dataset\n",
    "        all_pseudo_labeled_datasets.append(pseudo_dataset)\n",
    "\n",
    "        # Remove pseudo-labeled indices from remaining_unlabeled\n",
    "        # We need to track which indices from the original dataset were used\n",
    "        pseudo_indices = []\n",
    "        batch_idx = 0\n",
    "        for data, _ in unlabeled_loader:\n",
    "            outputs = model(data.to(device))\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probabilities, 1)\n",
    "            confident_mask = max_probs >= confidence_threshold\n",
    "            \n",
    "            for j, is_confident in enumerate(confident_mask):\n",
    "                if is_confident:\n",
    "                    if batch_idx * batch_size + j < len(remaining_unlabeled):\n",
    "                        global_idx = remaining_unlabeled[batch_idx * batch_size + j]\n",
    "                        pseudo_indices.append(global_idx)\n",
    "                        all_pseudo_labeled_indices.add(global_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "        # Update remaining unlabeled indices\n",
    "        remaining_unlabeled = [idx for idx in remaining_unlabeled if idx not in all_pseudo_labeled_indices]\n",
    "\n",
    "        # Combine original labeled data with ALL pseudo-labeled data so far\n",
    "        all_datasets = [original_train_dataset] + all_pseudo_labeled_datasets\n",
    "        combined_dataset = ConcatDataset(all_datasets)\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        print(f\" actual dataset number of samples {len(combined_dataset)}\")\n",
    "\n",
    "        # Dispose of the current model and clear GPU memory before reinitializing\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Reinitialize model and optimizer for combined training\n",
    "        model = BaselineModel().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # ensure learning_rate is defined\n",
    "        \n",
    "\n",
    "        print(f\"Training with {len(combined_dataset)} samples ({len(original_train_dataset)} original labeled + {sum(len(ds) for ds in all_pseudo_labeled_datasets)} pseudo-labeled)\")\n",
    "        print(f\"Remaining unlabeled samples: {len(remaining_unlabeled)}\")\n",
    "\n",
    "        best_model_path = f\"finetuned_model_iter{iteration}.pth\"\n",
    "        model = train_model(model, num_epochs, optimizer, combined_loader, val_data_loader, criterion, device, best_model_path)\n",
    "\n",
    "        \n",
    "        print(\"start evaluation : \")\n",
    "        model.eval()\n",
    "        test_loss, correct_predictions = validate(model, test_data_loader, criterion, device)\n",
    "        base_accuracy = correct_predictions / len(test_dataset)\n",
    "        print(f\"best model performance at iteratio {iteration} is : {base_accuracy} \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
