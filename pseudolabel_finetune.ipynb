{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,Subset, DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "from torchvision import transforms, datasets\n",
    "from pathlib import Path\n",
    "# split validation set into new train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "#plot examples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import copy\n",
    "\n",
    "from baselineCNN import *\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('./../wildfire-prediction-dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = dataset_path / 'train'\n",
    "val_path = dataset_path / 'valid'\n",
    "test_path = dataset_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(test_path, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(pretrain_path, val_path, test_path, transforms):\n",
    "    \n",
    "    pretrain_dataset = datasets.ImageFolder(pretrain_path, transform=transforms['pretrain'])\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=transforms['valid'])\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=transforms['test'])\n",
    "    train_idx, validation_idx = train_test_split(np.arange(len(val_dataset)),\n",
    "                                             test_size=0.2,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=val_dataset.targets)\n",
    "    train_dataset = Subset(val_dataset, train_idx)\n",
    "    val_dataset = Subset(val_dataset, validation_idx)\n",
    "    \n",
    "    return pretrain_dataset, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32  \n",
    "\n",
    "\n",
    "# Data transformations\n",
    "data_transforms = {\n",
    "    'pretrain': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "unlabeled, train_dataset, val_dataset, test_dataset = get_all_datasets(pretrain_path, val_path, test_path, data_transforms)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "unlabeled_loader = DataLoader(unlabeled, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, loss_fn, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x, y in tqdm(data_loader):\n",
    "        x = x.float().to(device).half()  # Convert to float16\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):  # Use automatic mixed precision\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "def train_model(model, num_epochs, optimizer, train_loader, val_loader, criterion, device, best_model_path):\n",
    "    model.train()\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, criterion, device)\n",
    "        val_loss, correct_predictions = validate(model, val_loader, criterion, device)\n",
    "        val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "            }, best_model_path)\n",
    "        \n",
    "        print(f'Train Loss: {np.mean(train_loss):.4f} Validation Loss: {np.mean(val_loss):.4f} Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def pseudo_label_dataset(model, unlabeled_loader, device, confidence_threshold=0.95):\n",
    "    model.eval()\n",
    "    pseudo_inputs = []\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(unlabeled_loader, desc=\"Generating pseudo-labels\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_probs, preds = torch.max(probabilities, 1)\n",
    "            \n",
    "            # Select samples with high confidence\n",
    "            confident_mask = max_probs >= confidence_threshold\n",
    "            if confident_mask.sum() > 0:\n",
    "                confident_inputs = inputs[confident_mask].cpu()\n",
    "                confident_labels = preds[confident_mask].cpu()\n",
    "                \n",
    "                pseudo_inputs.append(confident_inputs)\n",
    "                pseudo_labels.append(confident_labels)\n",
    "    \n",
    "    if not pseudo_inputs:\n",
    "        return None\n",
    "    \n",
    "    # Combine all selected samples\n",
    "    pseudo_inputs = torch.cat(pseudo_inputs, 0)\n",
    "    pseudo_labels = torch.cat(pseudo_labels, 0)\n",
    "    \n",
    "    pseudo_dataset = TensorDataset(pseudo_inputs, pseudo_labels.long())\n",
    "    print(f\"Generated {len(pseudo_dataset)} pseudo-labeled samples from {len(unlabeled_loader.dataset)} with a confidence threshold = {confidence_threshold}\")\n",
    "    return pseudo_dataset\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    # Convert all labels to a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3870708/1246028678.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"baseline.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline = baseline.to(device)\n",
    "checkpoint = torch.load(\"baseline.pth\")\n",
    "baseline.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:15<00:00, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1906 Test Accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "baseline.eval()\n",
    "test_loss, correct_predictions = validate(baseline, test_data_loader, criterion, device)\n",
    "base_accuracy = correct_predictions / len(test_dataset)\n",
    "print(f'Test Loss: {np.mean(test_loss):.4f} Test Accuracy: {base_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.98  # Initial confidence threshold\n",
    "initial_acc = base_accuracy\n",
    "num_iterations = 2\n",
    "num_epochs = 10\n",
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pseudo-labeling iteration :  1/2 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 946/946 [04:14<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11846 pseudo-labeled samples from 30250 with a confidence threshold = 0.98\n",
      " actual dataset number of samples 16886\n",
      "Training with 16886 samples (5040 original labeled + 11846 pseudo-labeled)\n",
      "Remaining unlabeled samples: 18404\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:47<00:00,  4.90it/s]\n",
      "100%|██████████| 40/40 [00:05<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1622 Validation Loss: 0.2134 Validation Accuracy: 0.9294\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:59<00:00,  4.43it/s]\n",
      "100%|██████████| 40/40 [00:05<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0694 Validation Loss: 0.2389 Validation Accuracy: 0.9341\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:54<00:00,  4.63it/s]\n",
      "100%|██████████| 40/40 [00:05<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0479 Validation Loss: 0.3217 Validation Accuracy: 0.8762\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [02:04<00:00,  4.24it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0462 Validation Loss: 0.2272 Validation Accuracy: 0.9476\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:59<00:00,  4.42it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0472 Validation Loss: 0.5369 Validation Accuracy: 0.7611\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:54<00:00,  4.61it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0502 Validation Loss: 0.3869 Validation Accuracy: 0.8659\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:53<00:00,  4.65it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0318 Validation Loss: 0.5977 Validation Accuracy: 0.7325\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:53<00:00,  4.67it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0245 Validation Loss: 0.3652 Validation Accuracy: 0.8405\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [02:02<00:00,  4.33it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0251 Validation Loss: 0.7014 Validation Accuracy: 0.7190\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [01:55<00:00,  4.57it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0178 Validation Loss: 0.5127 Validation Accuracy: 0.7770\n",
      "start evaluation : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:16<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model performance at iteratio 0 is : 0.7896825396825397 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pseudo-labeling iteration :  2/2 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 576/576 [01:47<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1573 pseudo-labeled samples from 18404 with a confidence threshold = 0.98\n",
      " actual dataset number of samples 18459\n",
      "Training with 18459 samples (5040 original labeled + 13419 pseudo-labeled)\n",
      "Remaining unlabeled samples: 16831\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [01:59<00:00,  4.85it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1624 Validation Loss: 0.4986 Validation Accuracy: 0.7103\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:12<00:00,  4.36it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0970 Validation Loss: 0.4079 Validation Accuracy: 0.7952\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:01<00:00,  4.75it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0691 Validation Loss: 0.2512 Validation Accuracy: 0.9087\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [01:58<00:00,  4.87it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0597 Validation Loss: 0.3219 Validation Accuracy: 0.8627\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:17<00:00,  4.19it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0467 Validation Loss: 0.4700 Validation Accuracy: 0.7548\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:10<00:00,  4.41it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0407 Validation Loss: 0.4686 Validation Accuracy: 0.7587\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:08<00:00,  4.49it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0451 Validation Loss: 0.8918 Validation Accuracy: 0.6198\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:23<00:00,  4.01it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0344 Validation Loss: 0.4514 Validation Accuracy: 0.7778\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:19<00:00,  4.13it/s]\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0240 Validation Loss: 0.3033 Validation Accuracy: 0.8651\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [02:04<00:00,  4.64it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0198 Validation Loss: 0.5306 Validation Accuracy: 0.7452\n",
      "start evaluation : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:16<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model performance at iteratio 1 is : 0.7598412698412699 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = baseline\n",
    "\n",
    "# Keep track of original labeled dataset\n",
    "original_train_dataset = train_dataset\n",
    "\n",
    "# Keep track of unlabeled samples and which ones have been pseudo-labeled\n",
    "remaining_unlabeled = list(range(len(unlabeled)))\n",
    "all_pseudo_labeled_indices = set()\n",
    "all_pseudo_labeled_datasets = []\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"\\nPseudo-labeling iteration :  {iteration+1}/{num_iterations} \\n \")\n",
    "    \n",
    "    # Create a loader only for remaining unlabeled data\n",
    "    remaining_unlabeled_dataset = torch.utils.data.Subset(unlabeled, remaining_unlabeled)\n",
    "    unlabeled_loader = DataLoader(remaining_unlabeled_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Generate pseudo-labels for the remaining unlabeled data\n",
    "    pseudo_dataset = pseudo_label_dataset(model, unlabeled_loader, device, confidence_threshold)\n",
    "    \n",
    "\n",
    "    if pseudo_dataset is None or len(pseudo_dataset) == 0:\n",
    "        print(f\"No confident samples found at threshold {confidence_threshold}. Lowering threshold.\")\n",
    "        confidence_threshold *= 0.9  # Reduce threshold\n",
    "\n",
    "    else : \n",
    "        # Store this iteration's pseudo-labeled dataset\n",
    "        all_pseudo_labeled_datasets.append(pseudo_dataset)\n",
    "\n",
    "        # Remove pseudo-labeled indices from remaining_unlabeled\n",
    "        # We need to track which indices from the original dataset were used\n",
    "        pseudo_indices = []\n",
    "        batch_idx = 0\n",
    "        for data, _ in unlabeled_loader:\n",
    "            outputs = model(data.to(device))\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probabilities, 1)\n",
    "            confident_mask = max_probs >= confidence_threshold\n",
    "            \n",
    "            for j, is_confident in enumerate(confident_mask):\n",
    "                if is_confident:\n",
    "                    if batch_idx * batch_size + j < len(remaining_unlabeled):\n",
    "                        global_idx = remaining_unlabeled[batch_idx * batch_size + j]\n",
    "                        pseudo_indices.append(global_idx)\n",
    "                        all_pseudo_labeled_indices.add(global_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "        # Update remaining unlabeled indices\n",
    "        remaining_unlabeled = [idx for idx in remaining_unlabeled if idx not in all_pseudo_labeled_indices]\n",
    "\n",
    "        # Combine original labeled data with ALL pseudo-labeled data so far\n",
    "        all_datasets = [original_train_dataset] + all_pseudo_labeled_datasets\n",
    "        combined_dataset = ConcatDataset(all_datasets)\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        print(f\" actual dataset number of samples {len(combined_dataset)}\")\n",
    "\n",
    "        # Dispose of the current model and clear GPU memory before reinitializing\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Reinitialize model and optimizer for combined training\n",
    "        model = BaselineModel().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # ensure learning_rate is defined\n",
    "        \n",
    "\n",
    "        print(f\"Training with {len(combined_dataset)} samples ({len(original_train_dataset)} original labeled + {sum(len(ds) for ds in all_pseudo_labeled_datasets)} pseudo-labeled)\")\n",
    "        print(f\"Remaining unlabeled samples: {len(remaining_unlabeled)}\")\n",
    "\n",
    "        best_model_path = f\"finetuned_model2_iter{iteration}.pth\"\n",
    "        model = train_model(model, num_epochs, optimizer, combined_loader, val_data_loader, criterion, device, best_model_path)\n",
    "\n",
    "        \n",
    "        print(\"start evaluation : \")\n",
    "        model.eval()\n",
    "        test_loss, correct_predictions = validate(model, test_data_loader, criterion, device)\n",
    "        base_accuracy = correct_predictions / len(test_dataset)\n",
    "        print(f\"best model performance at iteratio {iteration} is : {base_accuracy} \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
